"""
Pipeline Completo de Datos para SOL Trading Bot.

Este script conecta todos los pasos:
1. Lee datos RAW de market_scraper
2. Calcula indicadores t√©cnicos
3. Genera dataset etiquetado con targets

Ejecutar: python run_pipeline.py
"""

import os
import sys
import glob
from pathlib import Path

import polars as pl

# A√±adir src al path
sys.path.insert(0, str(Path(__file__).parent))

from src.processors.technical_indicators import FeatureEngineer
from src.quant_engine.dataset_builder import DatasetBuilder


def get_latest_raw_file():
    """Buscar el archivo parquet m√°s reciente en data/raw/market/"""
    files = glob.glob("data/raw/market/*.parquet")

    if not files:
        raise FileNotFoundError(
            "‚ùå No hay datos raw!\n"
            "   Ejecuta primero: python src/scrapers/market_scraper.py"
        )

    # Devolver el m√°s reciente por fecha de modificaci√≥n
    latest_file = max(files, key=os.path.getmtime)
    return latest_file


def main():
    print("\n" + "="*70)
    print("üöÄ PIPELINE DE DATOS - SOL TRADING BOT")
    print("="*70 + "\n")

    # PASO 1: Identificar archivo raw m√°s reciente
    print("PASO 1/3: Buscando datos raw...")
    print("-" * 70)

    try:
        raw_file = get_latest_raw_file()
        print(f"‚úÖ Archivo detectado: {raw_file}")

        # Cargar datos
        df_raw = pl.read_parquet(raw_file)
        print(f"   Registros: {len(df_raw):,}")
        print(f"   Columnas: {df_raw.columns}")
        print(f"   Per√≠odo: {df_raw['timestamp'].min()} ‚Üí {df_raw['timestamp'].max()}")

    except FileNotFoundError as e:
        print(f"\n{e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error leyendo archivo raw: {e}")
        sys.exit(1)

    # PASO 2: Calcular Indicadores T√©cnicos
    print("\n" + "="*70)
    print("PASO 2/3: Calculando indicadores t√©cnicos...")
    print("-" * 70)

    try:
        engineer = FeatureEngineer()

        print("Aplicando indicadores:")
        print("  ‚Ä¢ RSI (14)")
        print("  ‚Ä¢ ATR (14) - Crucial para el umbral din√°mico")
        print("  ‚Ä¢ EMA (50, 200)")
        print("  ‚Ä¢ Bollinger Bands (20, 2œÉ)")

        # Aplicar todos los indicadores
        df_processed = engineer.add_all_features(df_raw)

        initial_cols = len(df_raw.columns)
        final_cols = len(df_processed.columns)
        added_features = final_cols - initial_cols

        print(f"\n‚úÖ Indicadores calculados:")
        print(f"   Columnas originales: {initial_cols}")
        print(f"   Columnas finales: {final_cols}")
        print(f"   Features a√±adidas: {added_features}")

        # Guardar datos procesados
        processed_dir = Path("data/processed")
        processed_dir.mkdir(parents=True, exist_ok=True)

        processed_filename = "SOL_USDT_15m_indicators.parquet"
        processed_path = processed_dir / processed_filename

        df_processed.write_parquet(processed_path, compression="snappy")

        file_size_mb = processed_path.stat().st_size / (1024 * 1024)
        print(f"   Guardado en: {processed_path}")
        print(f"   Tama√±o: {file_size_mb:.2f} MB")

    except Exception as e:
        print(f"‚ùå Error calculando indicadores: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # PASO 3: Generar Dataset Etiquetado
    print("\n" + "="*70)
    print("PASO 3/3: Generando dataset etiquetado...")
    print("-" * 70)

    try:
        print("Configuraci√≥n de la estrategia 'Sniper':")
        print("  ‚Ä¢ Lookahead: 4 candles (1 hora en 15m)")
        print("  ‚Ä¢ Umbral: ATR √ó 1.5 (din√°mico por volatilidad)")
        print("  ‚Ä¢ Target: 1 si precio_futuro > precio_actual + umbral")
        print()

        builder = DatasetBuilder(
            processed_dir="data/processed",
            lookahead_candles=4,   # 1 hora
            atr_multiplier=1.5     # Umbral balanceado
        )

        # Construir dataset de entrenamiento
        # Nota: build_training_dataset espera solo el nombre del archivo, no la ruta completa
        final_dataset = builder.build_training_dataset(
            input_filename=processed_filename,
            output_filename="training_dataset.parquet",
            validate=True
        )

        print(f"\n‚úÖ Dataset final creado exitosamente!")
        print(f"   Ubicaci√≥n: data/processed/training_dataset.parquet")
        print(f"   Registros finales: {len(final_dataset):,}")

    except Exception as e:
        print(f"‚ùå Error generando dataset: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # RESUMEN FINAL
    print("\n" + "="*70)
    print("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
    print("="*70)
    print("\nüìÅ Archivos generados:")
    print(f"   1. {processed_path}")
    print(f"   2. data/processed/training_dataset.parquet")
    print("\nüéØ Pr√≥ximos pasos:")
    print("   1. Revisar la distribuci√≥n de clases (Buy Signals %)")
    print("   2. Ajustar atr_multiplier si es necesario (1.2-2.0)")
    print("   3. Dividir en train/val/test sets")
    print("   4. ¬°Comenzar a entrenar el modelo Transformer!")
    print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Pipeline interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error inesperado: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
